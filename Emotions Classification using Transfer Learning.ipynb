{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotions Classification using Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotions Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 16000 train values and 2000 test values, each containing two features\n",
    "\n",
    "**Objective** The objective is to predict the emotions bases on the text messages\n",
    "\n",
    "**Columns**\n",
    "\n",
    "1. Text- The text messages\n",
    "\n",
    "2. Label- The different emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing packages and Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Importing packages\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "# The Universal Sentence Encoder module\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "\n",
    "\n",
    "# Import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.Module(module_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Assigning messages using word, sentence, paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a representation for each message, showing various lengths supported.\n",
    "word = \"Levitization\"\n",
    "sentence = \"There are two types of Pain in this world: Pain that hurts you, and Pain that changes you\"\n",
    "paragraphs = (\n",
    "    \"Everyone is a genius. \"\n",
    "    \"But if you judge a fish on its ability to climb a tree,\" \n",
    "     \"it will live its whole life believing it is stupid. \")\n",
    "messages = [word, sentence, paragraphs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Performing Universal Sentence Encoding on the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Levitization\n",
      "Embedding size: 512\n",
      "Embedding: [-0.01299950573593378, -0.0030302153900265694, 0.024302732199430466, ...]\n",
      "\n",
      "Message: There are two types of Pain in this world: Pain that hurts you, and Pain that changes you\n",
      "Embedding size: 512\n",
      "Embedding: [0.031403250992298126, 0.04196187108755112, 0.014515469782054424, ...]\n",
      "\n",
      "Message: Everyone is a genius. But if you judge a fish on its ability to climb a tree,it will live its whole life believing it is stupid. \n",
      "Embedding size: 512\n",
      "Embedding: [0.03774511069059372, 0.07424656301736832, 0.06033611297607422, ...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Embed the messages using Universal Sentence Encoder\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "  message_embeddings = session.run(embed(messages))\n",
    "    \n",
    "# Print all the messages with its embedding and size  \n",
    "  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "    print(\"Message: {}\".format(messages[i]))\n",
    "    print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "    message_embedding_snippet = \", \".join(\n",
    "        (str(x) for x in message_embedding[:3]))\n",
    "    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of embedding\n",
    "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
    "embed_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reading the dataset  and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the datasets\n",
    "train = pd.read_csv('train.txt',sep=';',names=['text','label'])\n",
    "test = pd.read_csv('test.txt',sep=';',names=['text','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 2)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data                                                 text    label\n",
      "0                            i didnt feel humiliated  sadness\n",
      "1  i can go from feeling so hopeless to so damned...  sadness\n",
      "2   im grabbing a minute to post i feel greedy wrong    anger\n",
      "3  i am ever feeling nostalgic about the fireplac...     love\n",
      "4                               i am feeling grouchy    anger\n",
      "Test data                                                 text    label\n",
      "0  im feeling rather rotten so im not very ambiti...  sadness\n",
      "1          im updating my blog because i feel shitty  sadness\n",
      "2  i never make her separate from me because i do...  sadness\n",
      "3  i left with my bouquet of red and yellow tulip...      joy\n",
      "4    i was feeling a little vain when i did this one  sadness\n"
     ]
    }
   ],
   "source": [
    "# Getting the head of the datasets\n",
    "print('Train data',train.head())\n",
    "print('Test data',test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = train['label'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of categories in dataset\n",
    "category_counts = len(train.label.cat.categories)\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert train text to  an array and labels as get_dummies\n",
    "\n",
    "train_text = train['text'].tolist()\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "train_label = np.asarray(pd.get_dummies(train.label), dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert train text to  an array and labels as get_dummies\n",
    "\n",
    "test_text = test['text'].tolist()\n",
    "test_text = np.array(test_text,dtype=object)[:,np.newaxis]\n",
    "test_label = np.asarray(pd.get_dummies(test.label),dtype=np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Building the model using Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function for Universal Embedding \n",
    "def UniversalEmbedding(x):\n",
    "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 265,734\n",
      "Trainable params: 265,734\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Building\n",
    "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
    "embedding = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,))(input_text)\n",
    "dense = layers.Dense(512, activation='relu')(embedding)\n",
    "pred = layers.Dense(category_counts, activation='softmax')(dense)\n",
    "model = Model(inputs=[input_text], outputs=pred)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 967s 60ms/step - loss: 1.1157 - accuracy: 0.5870 - val_loss: 1.0059 - val_accuracy: 0.6275\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 954s 60ms/step - loss: 0.9804 - accuracy: 0.6331 - val_loss: 0.9795 - val_accuracy: 0.6235\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 955s 60ms/step - loss: 0.9215 - accuracy: 0.6573 - val_loss: 0.9396 - val_accuracy: 0.6555\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 955s 60ms/step - loss: 0.8636 - accuracy: 0.6798 - val_loss: 0.9214 - val_accuracy: 0.6450\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 949s 59ms/step - loss: 0.8071 - accuracy: 0.6991 - val_loss: 0.8922 - val_accuracy: 0.6530\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 956s 60ms/step - loss: 0.7549 - accuracy: 0.7222 - val_loss: 0.8834 - val_accuracy: 0.6665\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 972s 61ms/step - loss: 0.7067 - accuracy: 0.7427 - val_loss: 0.8758 - val_accuracy: 0.6705\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 991s 62ms/step - loss: 0.6590 - accuracy: 0.7614 - val_loss: 0.8724 - val_accuracy: 0.6765\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 960s 60ms/step - loss: 0.6137 - accuracy: 0.7764 - val_loss: 0.8843 - val_accuracy: 0.6695\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 957s 60ms/step - loss: 0.5726 - accuracy: 0.7933 - val_loss: 0.8807 - val_accuracy: 0.6745\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "with tf.Session() as session:\n",
    "  K.set_session(session)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  session.run(tf.tables_initializer())\n",
    "  history = model.fit(train_text, \n",
    "            train_label,\n",
    "            validation_data=(test_text, test_label),\n",
    "            epochs=10,\n",
    "            batch_size=32)\n",
    "  model.save_weights('./model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseSession._Callable.__del__ at 0x000001BCED47E488>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"C:\\Users\\sudha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness', 'joy', 'love', 'surprise', 'fear']\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "\n",
    "new_text = ['i just feel like all my efforts are in vain and a waste of time',\n",
    "            'i feel as if i am the beloved preparing herself for the wedding',\n",
    "           'i see the starlight caress your hair no more feel the tender kisses we used to share i close my eyes and clearly my heart remembers a thousand good byes could never put out the embers',\n",
    "           'i feel and im amazed of how often i think i need to save the world',\n",
    "            'i start to lose that sense of independence in that i feel a lot more hesitant to do things']\n",
    "\n",
    "new_text = np.array(new_text, dtype=object)[:, np.newaxis]\n",
    "with tf.Session() as session:\n",
    "  K.set_session(session)\n",
    "  session.run(tf.global_variables_initializer())\n",
    "  session.run(tf.tables_initializer())\n",
    "  model.load_weights('./model.h5')  \n",
    "  predicts = model.predict(new_text, batch_size=32)\n",
    "\n",
    "categories = train.label.cat.categories.tolist()\n",
    "predict_logits = predicts.argmax(axis=1)\n",
    "predict_labels = [categories[logit] for logit in predict_logits]\n",
    "print(predict_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
